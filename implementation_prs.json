{
  "project": "Continuous Batching for Llama Models (Rust + MLX)",
  "description": "Breaking down the implementation into logical PRs with clear dependencies",
  "prs": [
    {
      "pr_id": "PR-001",
      "title": "Add project dependencies and error handling infrastructure",
      "description": "Set up the foundational dependencies and error handling for the project. This includes adding mlx-rs, anyhow, and thiserror to Cargo.toml, and creating a comprehensive error type hierarchy that will be used throughout the codebase.",
      "scope": [
        "Update Cargo.toml with mlx-rs, anyhow, thiserror",
        "Create error types in lib.rs or separate error.rs module",
        "Define BatchingError enum with variants for MLX, sequence not found, invalid config, and model errors",
        "Add Result type alias"
      ],
      "files_created": [],
      "files_modified": [
        "Cargo.toml",
        "src/lib.rs"
      ],
      "estimated_complexity": "Low",
      "dependencies": []
    },
    {
      "pr_id": "PR-002",
      "title": "Implement model configuration structure",
      "description": "Create the ModelConfig struct that defines Llama model architecture parameters including support for Grouped Query Attention (GQA). This configuration will be used by all subsequent model and scheduler components.",
      "scope": [
        "Create src/config.rs with ModelConfig struct",
        "Add fields: vocab_size, n_layers, hidden_dim, n_heads, n_kv_heads, head_dim, intermediate_dim",
        "Add RoPE configuration: rope_base, rope_scale",
        "Add normalization config: rms_norm_eps",
        "Implement validation methods to ensure n_heads % n_kv_heads == 0 (GQA requirement)",
        "Add constructor with validation"
      ],
      "files_created": [
        "src/config.rs"
      ],
      "files_modified": [
        "src/lib.rs"
      ],
      "estimated_complexity": "Low",
      "dependencies": ["PR-001"]
    },
    {
      "pr_id": "PR-003",
      "title": "Implement KV cache data structures",
      "description": "Build the per-sequence KV cache infrastructure that will store key-value pairs for each layer of the model. This is a critical component for continuous batching as each sequence maintains its own cache that grows during generation.",
      "scope": [
        "Create src/kv_cache.rs",
        "Define LayerKVCache struct with Option<mlx_rs::Array> for K and V tensors",
        "Define SequenceKVCache with Vec<LayerKVCache> (one per layer)",
        "Implement new() to initialize empty cache for n_layers",
        "Implement set_layer() to store K/V tensors for a specific layer",
        "Implement get_layer() to retrieve K/V tensors",
        "Implement append_layer() to concatenate new K/V with existing cache",
        "Implement current_length() to get sequence length from cache",
        "Add documentation explaining tensor shapes: [n_kv_heads, seq_len, head_dim]"
      ],
      "files_created": [
        "src/kv_cache.rs"
      ],
      "files_modified": [
        "src/lib.rs"
      ],
      "estimated_complexity": "Medium",
      "dependencies": ["PR-001"]
    },
    {
      "pr_id": "PR-004",
      "title": "Implement sequence state management",
      "description": "Create the Sequence struct and related types that track the lifecycle of each generation request. This includes token tracking, position management, stopping conditions, and integration with KV cache.",
      "scope": [
        "Create src/sequence.rs",
        "Define SeqId type alias (u64)",
        "Define SequenceStatus enum: Waiting, Running, Completed",
        "Define Sequence struct with all required fields",
        "Implement new() constructor",
        "Implement is_finished() to check stopping conditions (EOS or max_new_tokens)",
        "Implement current_length() helper (prompt + generated tokens)",
        "Implement append_token() to add generated token and increment next_pos",
        "Add unit tests for sequence lifecycle"
      ],
      "files_created": [
        "src/sequence.rs"
      ],
      "files_modified": [
        "src/lib.rs"
      ],
      "estimated_complexity": "Medium",
      "dependencies": ["PR-001", "PR-003"]
    },
    {
      "pr_id": "PR-005",
      "title": "Implement TGI-style continuous batching scheduler",
      "description": "Build the core scheduling logic that implements TGI's continuous batching algorithm. The scheduler dynamically composes batches by filling available slots with prefills while maintaining all running sequences for decode.",
      "scope": [
        "Create src/scheduler.rs",
        "Define Scheduler struct with waiting_queue, running_batch, sequences HashMap",
        "Define SchedulerDecision enum with Batch and Idle variants",
        "Implement new() constructor with max_batch_size parameter",
        "Implement add_sequence() to create and queue new sequences",
        "Implement schedule() with TGI algorithm: fill batch up to max_batch_size",
        "Implement add_to_running_batch() to move sequence from waiting to running",
        "Implement complete_sequence() to remove from running_batch and mark completed",
        "Implement has_work() to check if any sequences remain",
        "Implement get_sequence() and get_sequence_mut() helpers",
        "Add unit tests for scheduling decisions with various queue states"
      ],
      "files_created": [
        "src/scheduler.rs"
      ],
      "files_modified": [
        "src/lib.rs"
      ],
      "estimated_complexity": "High",
      "dependencies": ["PR-001", "PR-004"]
    },
    {
      "pr_id": "PR-006",
      "title": "Add model interface and stub implementation",
      "description": "Define the LlamaModel interface with prefill() and decode_step() methods. Initially implement with stubs that return dummy tensors, allowing the scheduler and worker to be tested independently of actual model computation.",
      "scope": [
        "Create src/model.rs",
        "Define LlamaModel struct with ModelConfig field",
        "Implement new() constructor",
        "Define prefill() method signature returning (Array, SequenceKVCache)",
        "Define decode_step() method signature returning Array",
        "Add stub implementations that return zeros/dummy tensors with correct shapes",
        "Add extensive documentation on GQA and RoPE requirements",
        "Add TODO comments for actual implementation in PR-008"
      ],
      "files_created": [
        "src/model.rs"
      ],
      "files_modified": [
        "src/lib.rs"
      ],
      "estimated_complexity": "Medium",
      "dependencies": ["PR-001", "PR-002", "PR-003"]
    },
    {
      "pr_id": "PR-007",
      "title": "Implement model worker and execution loop",
      "description": "Create the ModelWorker that orchestrates the scheduler and model, implementing the main execution loop. This ties together all previous components and implements the step-by-step generation logic following TGI's continuous batching approach.",
      "scope": [
        "Create src/worker.rs",
        "Define ModelWorker struct with model and scheduler fields",
        "Define GeneratedToken struct for output",
        "Implement new() constructor",
        "Implement add_sequence() proxy to scheduler",
        "Implement step() method with full TGI logic: prefill loop + decode batch",
        "Implement run_until_complete() that calls step() until no work remains",
        "Implement greedy sampling helper (argmax)",
        "Add comprehensive error handling",
        "Add integration tests with stub model"
      ],
      "files_created": [
        "src/worker.rs"
      ],
      "files_modified": [
        "src/lib.rs"
      ],
      "estimated_complexity": "High",
      "dependencies": ["PR-001", "PR-005", "PR-006"]
    },
    {
      "pr_id": "PR-008",
      "title": "Implement Llama model forward pass with GQA and RoPE",
      "description": "Replace stub implementations with actual Llama model forward pass. This includes attention with Grouped Query Attention, RoPE embeddings, feedforward layers, and proper KV cache management. This is the most complex PR as it involves heavy MLX tensor operations.",
      "scope": [
        "Implement RoPE (Rotary Position Embedding) helper functions",
        "Implement RMSNorm layer",
        "Implement GQA attention mechanism with KV head expansion",
        "Implement feedforward/MLP layer with SwiGLU activation",
        "Implement single transformer layer",
        "Implement prefill() with full causal attention over prompt",
        "Implement decode_step() with batched single-token forward pass",
        "Implement proper KV cache append logic for each layer",
        "Add detailed comments explaining tensor shapes at each step",
        "Note: Actual weight loading is out of scope; focus on forward pass logic"
      ],
      "files_created": [],
      "files_modified": [
        "src/model.rs"
      ],
      "estimated_complexity": "Very High",
      "dependencies": ["PR-006"]
    },
    {
      "pr_id": "PR-009",
      "title": "Add comprehensive integration tests and examples",
      "description": "Create end-to-end tests that validate the entire continuous batching pipeline. Add example programs demonstrating usage patterns.",
      "scope": [
        "Create tests/integration_test.rs",
        "Test single sequence generation end-to-end",
        "Test multiple sequences with interleaved prefill/decode",
        "Test stopping conditions (EOS and max_tokens)",
        "Test dynamic batch composition (sequences arriving at different times)",
        "Test edge cases (empty queue, full batch, etc.)",
        "Create examples/basic_usage.rs showing API usage",
        "Add README.md with usage documentation"
      ],
      "files_created": [
        "tests/integration_test.rs",
        "examples/basic_usage.rs",
        "README.md"
      ],
      "files_modified": [],
      "estimated_complexity": "Medium",
      "dependencies": ["PR-007", "PR-008"]
    },
    {
      "pr_id": "PR-010",
      "title": "Add advanced sampling strategies (temperature, top-k, top-p)",
      "description": "Extend beyond greedy sampling to support temperature-based sampling, top-k filtering, and nucleus (top-p) sampling. This makes the generation more flexible and production-ready.",
      "scope": [
        "Create src/sampling.rs module",
        "Define SamplingParams struct with temperature, top_k, top_p fields",
        "Implement temperature scaling",
        "Implement top-k filtering",
        "Implement top-p (nucleus) filtering",
        "Implement sample() function that combines all strategies",
        "Update ModelWorker to accept SamplingParams per sequence",
        "Add tests for each sampling strategy"
      ],
      "files_created": [
        "src/sampling.rs"
      ],
      "files_modified": [
        "src/lib.rs",
        "src/worker.rs",
        "src/sequence.rs"
      ],
      "estimated_complexity": "Medium",
      "dependencies": ["PR-007"]
    },
    {
      "pr_id": "PR-011",
      "title": "Add performance monitoring and metrics",
      "description": "Add optional performance tracking to measure throughput, latency, and batch utilization. This helps understand the effectiveness of continuous batching without adding serving infrastructure.",
      "scope": [
        "Create src/metrics.rs module",
        "Define Metrics struct tracking: total_tokens_generated, prefill_count, decode_steps, average_batch_size",
        "Add timing measurements for prefill and decode operations",
        "Implement throughput calculation (tokens/sec)",
        "Add metrics to ModelWorker",
        "Implement report() method to print statistics",
        "Add optional feature flag for metrics to avoid overhead in production"
      ],
      "files_created": [
        "src/metrics.rs"
      ],
      "files_modified": [
        "Cargo.toml",
        "src/lib.rs",
        "src/worker.rs"
      ],
      "estimated_complexity": "Low",
      "dependencies": ["PR-007"]
    }
  ],
  "summary": {
    "total_prs": 11,
    "critical_path": [
      "PR-001",
      "PR-002",
      "PR-003",
      "PR-004",
      "PR-005",
      "PR-006",
      "PR-007",
      "PR-008",
      "PR-009"
    ],
    "parallel_opportunities": [
      "PR-002 and PR-003 can be done in parallel after PR-001",
      "PR-010 and PR-011 can be done in parallel after PR-007"
    ]
  }
}
